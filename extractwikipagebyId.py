import requests

def get_wiki_page_by_id(page_id: int):
    url = "https://vi.wikipedia.org/w/api.php"
    params = {
        "action": "query",
        "pageids": page_id,
        "prop": "extracts|info|categories|pageimages|langlinks|extlinks|templates|links",
        "inprop": "url",
        "pithumbsize": 500,  # for thumbnails
        "explaintext": True,
        "cllimit": "max",
        "format": "json"
    }

    headers = {
        "User-Agent": "MyWikiFetcher/1.0 (contact: your_email@example.com)"
    }

    response = requests.get(url, params=params, headers=headers)

    # âœ… Check HTTP status first
    if response.status_code != 200:
        print(f"âŒ HTTP error: {response.status_code}")
        print(response.text[:500])  # show first part of response
        exit()

    # âœ… Ensure response contains JSON
    if not response.text.strip().startswith("{"):
        print("âŒ Response is not JSON (maybe blocked or empty):")
        print(response.text)
        exit()

    # Parse JSON safely
    data = response.json()

    # TrÃ­ch xuáº¥t dá»¯ liá»‡u trang tá»« JSON
    pages = data.get("query", {}).get("pages", {})
    if not pages:
        return None

    page_data = next(iter(pages.values()))  # láº¥y pháº§n tá»­ Ä‘áº§u tiÃªn
    return page_data


if __name__ == "__main__":
    page_id = 375318  # VÃ­ dá»¥: trang "Internet Society"
    page = get_wiki_page_by_id(page_id)
    
    if page:
        print("âœ… TiÃªu Ä‘á»:", page["title"])
        print("ğŸ”— URL:", page["fullurl"])
        print("\nğŸ“œ Ná»™i dung tÃ³m táº¯t:\n")
        print(page["extract"][:800], "...")

        # âœ… Save to JSON file
        filename = f"wiki_page_{page_id}.json"
        import json
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(page, f, ensure_ascii=False, indent=4)

        print(f"\nğŸ’¾ ÄÃ£ lÆ°u dá»¯ liá»‡u vÃ o: {filename}")

    else:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y trang!")
